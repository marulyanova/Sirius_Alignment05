{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "643e7da1-47f1-4998-b136-67baf873c99a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-11T11:33:30.739734Z",
     "iopub.status.busy": "2024-10-11T11:33:30.739349Z",
     "iopub.status.idle": "2024-10-11T11:33:35.197042Z",
     "shell.execute_reply": "2024-10-11T11:33:35.196300Z",
     "shell.execute_reply.started": "2024-10-11T11:33:30.739714Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b213eb3-69cc-4f4a-b57a-96309cf69545",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-11T10:55:53.221031Z",
     "iopub.status.busy": "2024-10-11T10:55:53.220241Z",
     "iopub.status.idle": "2024-10-11T10:55:55.878419Z",
     "shell.execute_reply": "2024-10-11T10:55:55.877726Z",
     "shell.execute_reply.started": "2024-10-11T10:55:53.221004Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install peft transformers protobuf==3.20 bitsandbytes sentencepiece sacrebleu ipython datasets evaluate deepspeed einops  wandb zstandard accelerate jsonlines trl -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b74706ad-a72e-4a7b-bf35-de219fe655b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-11T10:55:55.880321Z",
     "iopub.status.busy": "2024-10-11T10:55:55.879560Z",
     "iopub.status.idle": "2024-10-11T10:55:55.902580Z",
     "shell.execute_reply": "2024-10-11T10:55:55.902020Z",
     "shell.execute_reply.started": "2024-10-11T10:55:55.880301Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !git clone https://huggingface.co/haoranxu/ALMA-7B /home/jupyter/datasphere/project/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "270c14fa-b6d4-499b-b0ad-6b5ec4e1a549",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-11T10:55:55.903466Z",
     "iopub.status.busy": "2024-10-11T10:55:55.903217Z",
     "iopub.status.idle": "2024-10-11T10:56:05.638178Z",
     "shell.execute_reply": "2024-10-11T10:56:05.637479Z",
     "shell.execute_reply.started": "2024-10-11T10:55:55.903447Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import os\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/home/jupyter/my_cache/huggingface\"\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "181847bd-7325-4e6e-9c8f-b93540796f36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-11T10:59:30.868980Z",
     "iopub.status.busy": "2024-10-11T10:59:30.868611Z",
     "iopub.status.idle": "2024-10-11T11:06:42.368890Z",
     "shell.execute_reply": "2024-10-11T11:06:42.368123Z",
     "shell.execute_reply.started": "2024-10-11T10:59:30.868959Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 6/6 [00:00<00:00, 635.16it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [06:59<00:00, 69.95s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"haoranxu/X-ALMA-13B-Pretrain\", torch_dtype=torch.float16, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"haoranxu/X-ALMA-13B-Pretrain\", padding_side='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca459d6b-7810-4667-87e7-58c082d1a33b",
   "metadata": {},
   "source": [
    "Подключению модели и токенайзера отработало 13:11, 11 октября"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef0f7cc-839a-4389-9b64-de19fc280d8f",
   "metadata": {},
   "source": [
    "# Чиню ошибку подключения гпу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00383ae3-9779-486b-8c7f-9c8f7a965325",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-11T10:59:18.026548Z",
     "iopub.status.busy": "2024-10-11T10:59:18.025972Z",
     "iopub.status.idle": "2024-10-11T10:59:18.047329Z",
     "shell.execute_reply": "2024-10-11T10:59:18.046788Z",
     "shell.execute_reply.started": "2024-10-11T10:59:18.026523Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())  # Должно вернуть True, если GPU доступен\n",
    "print(torch.cuda.device_count())  # Количество доступных GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccd219f2-6124-482c-9440-21f690c7c411",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-11T11:06:48.346145Z",
     "iopub.status.busy": "2024-10-11T11:06:48.345625Z",
     "iopub.status.idle": "2024-10-11T11:06:48.357890Z",
     "shell.execute_reply": "2024-10-11T11:06:48.357134Z",
     "shell.execute_reply.started": "2024-10-11T11:06:48.346124Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  24945 MiB |  24945 MiB |  24945 MiB |    512 B   |\n",
      "|       from large pool |  24945 MiB |  24945 MiB |  24945 MiB |      0 B   |\n",
      "|       from small pool |      0 MiB |      0 MiB |      0 MiB |    512 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  24945 MiB |  24945 MiB |  24945 MiB |    512 B   |\n",
      "|       from large pool |  24945 MiB |  24945 MiB |  24945 MiB |      0 B   |\n",
      "|       from small pool |      0 MiB |      0 MiB |      0 MiB |    512 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |  24825 MiB |  24825 MiB |  24825 MiB |      8 B   |\n",
      "|       from large pool |  24825 MiB |  24825 MiB |  24825 MiB |      0 B   |\n",
      "|       from small pool |      0 MiB |      0 MiB |      0 MiB |      8 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  24950 MiB |  24950 MiB |  24952 MiB |   2048 KiB |\n",
      "|       from large pool |  24948 MiB |  24948 MiB |  24948 MiB |      0 KiB |\n",
      "|       from small pool |      2 MiB |      2 MiB |      4 MiB |   2048 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   4289 KiB |   4360 KiB |   7157 KiB |   2868 KiB |\n",
      "|       from large pool |   3072 KiB |   3072 KiB |   3072 KiB |      0 KiB |\n",
      "|       from small pool |   1217 KiB |   2047 KiB |   4085 KiB |   2868 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     404    |     404    |     405    |       1    |\n",
      "|       from large pool |     282    |     282    |     282    |       0    |\n",
      "|       from small pool |     122    |     122    |     123    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     404    |     404    |     405    |       1    |\n",
      "|       from large pool |     282    |     282    |     282    |       0    |\n",
      "|       from small pool |     122    |     122    |     123    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     283    |     283    |     284    |       1    |\n",
      "|       from large pool |     282    |     282    |     282    |       0    |\n",
      "|       from small pool |       1    |       1    |       2    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       3    |       3    |       4    |       1    |\n",
      "|       from large pool |       2    |       2    |       2    |       0    |\n",
      "|       from small pool |       1    |       1    |       2    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# скопируйте эту ячейку, чтобы посмотреть расход памяти гпу\n",
    "# это нужно только для отладки\n",
    "summary = torch.cuda.memory_summary(device=None, abbreviated=False)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429c8075-1c14-42a5-b4c6-1d657a1f211c",
   "metadata": {},
   "source": [
    "13:11, 11 октября\n",
    "отработало\n",
    "```\n",
    "model = AutoModelForCausalLM.from_pretrained(\"haoranxu/X-ALMA-13B-Pretrain\", torch_dtype=torch.float16, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"haoranxu/X-ALMA-13B-Pretrain\", padding_side='left')\n",
    "```\n",
    "\n",
    "мнение ГПТ от расходе памяти\n",
    "\n",
    "24.5 ГБ памяти на GPU – это довольно высокий расход, особенно для моделей, использующихся в NLP (Natural Language Processing), таких как X-ALMA. Это значение указывает на то, что ваша модель, вероятно, большая и сложная, учитывая, что типичная модель может потреблять от нескольких гигабайт до 30 ГБ в зависимости от архитектуры и настроек.\n",
    "\n",
    "Возможные проблемы\n",
    "Kernel Out Of Memory (OOM): Если ваша модель превышает доступную память, это приведет к ошибке OOM. Ваша модель может потребовать оптимизации или использования другой конфигурации (например, уменьшение размера батча или использование режима \"offload\").\n",
    "\n",
    "Оптимизация памяти: Вы можете рассмотреть возможность использования более эффективных настроек памяти, таких как:\n",
    "\n",
    "Уменьшение размера батча.\n",
    "Использование смешанной точности (например, bfloat16 или float16).\n",
    "Отключение несущественных частей модели или использование более легких версий."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca8b301-9a56-444c-817c-c038f068bbaa",
   "metadata": {},
   "source": [
    "# Проверка работоспособности альмы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "910ba0b3-bfec-4c90-9924-8829368eb25b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-11T11:06:54.248640Z",
     "iopub.status.busy": "2024-10-11T11:06:54.248254Z",
     "iopub.status.idle": "2024-10-11T11:06:59.486570Z",
     "shell.execute_reply": "2024-10-11T11:06:59.485820Z",
     "shell.execute_reply.started": "2024-10-11T11:06:54.248620Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Translate this from English to Russian:\\nEnglish: Hello, how are you!\\nRussian: Привет, как дела?\\nTranslate this from Russian to English:\\nRussian: При']\n"
     ]
    }
   ],
   "source": [
    "# Add the source sentence into the prompt template\n",
    "prompt=\"Translate this from English to Russian:\\nEnglish: Hello, how are you!\\nRussian:\"\n",
    "\n",
    "# X-ALMA needs chat template but ALMA and ALMA-R don't need it.\n",
    "# chat_style_prompt = [{\"role\": \"user\", \"content\": prompt}]\n",
    "# prompt = tokenizer.apply_chat_template(chat_style_prompt, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\", padding=True, max_length=40, truncation=True).input_ids.cuda()\n",
    "\n",
    "# Translation\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(input_ids=input_ids, num_beams=5, max_new_tokens=20, do_sample=True, temperature=0.8, top_p=0.9)\n",
    "outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30bab80-1d9f-4e2a-a418-b533e24e221e",
   "metadata": {},
   "source": [
    "Токенизация: Здесь строка prompt преобразуется в идентификаторы токенов, которые могут быть обработаны моделью.\n",
    "return_tensors=\"pt\" указывает на то, что возвращаемые данные должны быть в формате, совместимом с PyTorch.\n",
    "padding=True добавляет необходимое выравнивание.\n",
    "max_length=40 и truncation=True устанавливают максимальную длину последовательности, чтобы избежать переполнения.\n",
    ".cuda() перемещает данные на GPU для ускорения обработки.\n",
    "\n",
    "torch.no_grad() отключает градиенты, что экономит память и вычислительные ресурсы, так как мы не планируем обновлять модель во время генерации.\n",
    "model.generate(...) запускает процесс генерации текста.\n",
    "input_ids=input_ids: входные идентификаторы, которые мы токенизировали ранее.\n",
    "num_beams=5: метод поиска с использованием 5 лучей, что улучшает качество генерации.\n",
    "max_new_tokens=20: максимальное количество токенов, которые будут сгенерированы.\n",
    "do_sample=True: включает стохастическую выборку.\n",
    "temperature=0.8: регулирует случайность при выборе следующего токена (меньшее значение — более предсказуемый выбор).\n",
    "top_p=0.9: ограничивает выбор токенов до тех, которые составляют 90% от вероятности, что также добавляет разнообразие в выбор.\n",
    "\n",
    "Декодирование результатов: Здесь сгенерированные идентификаторы переводятся обратно в текст с помощью токенизатора.\n",
    "skip_special_tokens=True игнорирует специальные токены, которые могут быть включены в результат, например, <s> или </s>.\n",
    "\n",
    "Что можно сделать дальше?\n",
    "Увеличить max_new_tokens: Попробуйте увеличить max_new_tokens, чтобы получить полный перевод.\n",
    "Экспериментировать с temperature и top_p: Попробуйте разные значения, чтобы увидеть, как они влияют на качество перевода.\n",
    "Тестировать с другими предложениями: Попробуйте перевести различные предложения для оценки качества модели в разных контекстах.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ae80afc-9e00-4c31-a733-80a2683190d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T13:57:05.632043Z",
     "iopub.status.busy": "2024-10-10T13:57:05.631663Z",
     "iopub.status.idle": "2024-10-10T13:57:05.662078Z",
     "shell.execute_reply": "2024-10-10T13:57:05.661562Z",
     "shell.execute_reply.started": "2024-10-10T13:57:05.632024Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# вы с ума сошли? \n",
    "# вы три дня чините ноутбук, где просите игнорировать сообщения об ошибке...\n",
    "# import warnings\n",
    "\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7601616-d7e6-48d8-9d87-bd1702a2cc9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T12:37:28.764076Z",
     "iopub.status.busy": "2024-10-08T12:37:28.762968Z",
     "iopub.status.idle": "2024-10-08T12:37:45.658968Z",
     "shell.execute_reply": "2024-10-08T12:37:45.658139Z",
     "shell.execute_reply.started": "2024-10-08T12:37:28.764025Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'ALMA'...\n",
      "Updating files: 100% (220/220), done.\n"
     ]
    }
   ],
   "source": [
    "# !git clone https://github.com/fe1ixxu/ALMA.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d38184d1-8461-4955-9aac-544b5da13606",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-11T11:07:06.908802Z",
     "iopub.status.busy": "2024-10-11T11:07:06.908392Z",
     "iopub.status.idle": "2024-10-11T11:07:06.926681Z",
     "shell.execute_reply": "2024-10-11T11:07:06.926001Z",
     "shell.execute_reply.started": "2024-10-11T11:07:06.908783Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Конфигурация сохранена в файл: /home/jupyter/datasphere/project/ALMA/configs/conf_axinia.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Определяем конфигурацию в виде словаря\n",
    "config = {\n",
    "    \"compute_environment\": \"LOCAL_MACHINE\",\n",
    "    \"deepspeed_config\": {\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        \"gradient_clipping\": 1.0,\n",
    "        \"offload_optimizer_device\": \"cuda\",\n",
    "        \"offload_param_device\": \"cpu\",\n",
    "        \"zero3_init_flag\": False,\n",
    "        \"zero_stage\": 2\n",
    "    },\n",
    "    \"distributed_type\": \"DEEPSPEED\",\n",
    "    \"downcast_bf16\": \"no\",\n",
    "    \"machine_rank\": 0,\n",
    "    \"main_training_function\": \"main\",\n",
    "    \"mixed_precision\": \"bf16\",\n",
    "    \"num_machines\": 1,\n",
    "    \"num_processes\": 8,\n",
    "    \"rdzv_backend\": \"static\",\n",
    "    \"same_network\": True,\n",
    "    \"tpu_env\": [],\n",
    "    \"tpu_use_cluster\": False,\n",
    "    \"tpu_use_sudo\": False,\n",
    "    \"use_cpu\": True,\n",
    "    \"model_type\": 'llama'\n",
    "}\n",
    "\n",
    "# Указываем путь к файлу, в который будем сохранять конфигурацию\n",
    "json_file_path = '/home/jupyter/datasphere/project/ALMA/configs/conf_axinia.json'\n",
    "\n",
    "# Сохраняем конфигурацию в JSON файл\n",
    "with open(json_file_path, 'w') as json_file:\n",
    "    json.dump(config, json_file, indent=4)\n",
    "\n",
    "print(f\"Конфигурация сохранена в файл: {json_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a67d46-db10-4589-a4ac-6c2ee416493d",
   "metadata": {},
   "source": [
    "Описание ключей конфигурации:\n",
    "compute_environment: Указывает среду выполнения. Значение \"LOCAL_MACHINE\" говорит о том, что код будет выполняться на локальном компьютере.\n",
    "\n",
    "deepspeed_config: Содержит параметры конфигурации для библиотеки DeepSpeed, которая оптимизирует обучение больших моделей.\n",
    "\n",
    "gradient_accumulation_steps: Количество шагов накопления градиентов. Установка в 1 означает, что градиенты будут обновляться каждый шаг.\n",
    "gradient_clipping: Значение для обрезки градиентов, чтобы предотвратить взрыв градиентов. Установка в 1.0 ограничивает максимальное значение градиента.\n",
    "offload_optimizer_device: Устройство для переноса оптимизатора. Установка в \"cuda\" указывает на использование GPU для хранения данных оптимизатора.\n",
    "offload_param_device: Устройство для переноса параметров модели. Установка в \"cpu\" означает, что параметры будут храниться на центральном процессоре.\n",
    "zero3_init_flag: Флаг для инициализации режима Zero Redundancy Optimizer (ZeRO) на уровне 3. Установка в False говорит о том, что этот режим не будет активирован.\n",
    "zero_stage: Уровень ZeRO, который будет использоваться. Значение 2 указывает на определённый уровень оптимизации.\n",
    "distributed_type: Указывает тип распределённого обучения. В данном случае \"DEEPSPEED\" означает, что будет использоваться библиотека DeepSpeed для распределённого обучения.\n",
    "\n",
    "downcast_bf16: Параметр, который указывает, будет ли использоваться понижение точности для bf16. Значение \"no\" указывает на то, что это не будет сделано.\n",
    "\n",
    "machine_rank: Порядковый номер машины в распределённой системе. Установка в 0 говорит о том, что это первая (и, возможно, единственная) машина.\n",
    "\n",
    "main_training_function: Указывает основную функцию для обучения. Значение \"main\" указывает на то, что основной код для обучения будет находиться в функции с именем main.\n",
    "\n",
    "mixed_precision: Указывает тип смешанной точности. Значение \"bf16\" означает, что будет использоваться 16-битная точность для обучения, что может ускорить вычисления на современных GPU.\n",
    "num_machines: Общее количество машин в распределённой системе. Установка в 1 говорит о том, что используется только одна машина.\n",
    "\n",
    "num_processes: Общее количество процессов, которые будут использоваться для обучения. Установка в 8 предполагает использование восьми процессов.\n",
    "\n",
    "rdzv_backend: Указывает бэкенд для координации распределённого обучения. \"static\" указывает на статическую настройку.\n",
    "\n",
    "same_network: Логическое значение, указывающее, находятся ли все машины в одной сети. Установка в True указывает на это.\n",
    "\n",
    "tpu_env: Список, указывающий окружение TPU. Пустой список указывает, что TPU не используется.\n",
    "tpu_use_cluster: Логическое значение, указывающее, будет ли использоваться кластер TPU. Установка в False означает, что кластер не используется.\n",
    "\n",
    "tpu_use_sudo: Логическое значение, указывающее, будет ли использоваться sudo для TPU. Установка в False означает, что sudo не будет использоваться.\n",
    "\n",
    "use_cpu: Логическое значение, указывающее, будет ли использоваться CPU. Установка в True означает, что CPU будет использован.\n",
    "\n",
    "model_type: Указывает тип модели, используемой для обучения. Значение 'llama' указывает на то, что будет использоваться модель LLaMA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a8efba5-1eb5-47d2-afa8-fbb5a1b33264",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-11T11:07:14.229820Z",
     "iopub.status.busy": "2024-10-11T11:07:14.229424Z",
     "iopub.status.idle": "2024-10-11T11:07:40.526513Z",
     "shell.execute_reply": "2024-10-11T11:07:40.525923Z",
     "shell.execute_reply.started": "2024-10-11T11:07:14.229800Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-11 11:07:19.912052: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-11 11:07:24.287105: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/11/2024 11:07:34 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n",
      "10/11/2024 11:07:34 - INFO - __main__ - Training/evaluation parameters CPOConfig(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "batch_eval_metrics=False,\n",
      "beta=0.1,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "cpo_alpha=1.0,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "dataset_num_proc=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_dropout=True,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=1,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=no,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generate_during_eval=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "is_encoder_decoder=None,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_pad_token_id=-100,\n",
      "label_smoothing=0.0,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=dist/runs/Oct11_11-07-34_g21-d3063012-d8d4-45fa-8730-ab3288050610,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=0.05,\n",
      "logging_strategy=steps,\n",
      "loss_type=sigmoid,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=inverse_sqrt,\n",
      "max_completion_length=None,\n",
      "max_grad_norm=1.0,\n",
      "max_length=512,\n",
      "max_prompt_length=256,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "model_init_kwargs=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=dist,\n",
      "overwrite_output_dir=True,\n",
      "padding_value=None,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=True,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=dist,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=1,\n",
      "seed=42,\n",
      "simpo_gamma=0.5,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "truncation_mode=keep_end,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.01,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.01,\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Using custom data configuration default-31a8eb8e26a3d2e8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/11/2024 11:07:36 - INFO - datasets.builder - Using custom data configuration default-31a8eb8e26a3d2e8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overwrite dataset info from restored data version if exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/11/2024 11:07:36 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset info from /home/jupyter/datasphere/project/datasetscache/marulyanova___alma-r-preference-f/default-31a8eb8e26a3d2e8/0.0.0/e0b54ddc5efda729a00ee89c8215eb1ed5e14423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/11/2024 11:07:36 - INFO - datasets.info - Loading Dataset info from /home/jupyter/datasphere/project/datasetscache/marulyanova___alma-r-preference-f/default-31a8eb8e26a3d2e8/0.0.0/e0b54ddc5efda729a00ee89c8215eb1ed5e14423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset alma-r-preference-f (/home/jupyter/datasphere/project/datasetscache/marulyanova___alma-r-preference-f/default-31a8eb8e26a3d2e8/0.0.0/e0b54ddc5efda729a00ee89c8215eb1ed5e14423)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/11/2024 11:07:36 - INFO - datasets.builder - Found cached dataset alma-r-preference-f (/home/jupyter/datasphere/project/datasetscache/marulyanova___alma-r-preference-f/default-31a8eb8e26a3d2e8/0.0.0/e0b54ddc5efda729a00ee89c8215eb1ed5e14423)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset info from /home/jupyter/datasphere/project/datasetscache/marulyanova___alma-r-preference-f/default-31a8eb8e26a3d2e8/0.0.0/e0b54ddc5efda729a00ee89c8215eb1ed5e14423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/11/2024 11:07:36 - INFO - datasets.info - Loading Dataset info from /home/jupyter/datasphere/project/datasetscache/marulyanova___alma-r-preference-f/default-31a8eb8e26a3d2e8/0.0.0/e0b54ddc5efda729a00ee89c8215eb1ed5e14423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:2206] 2024-10-11 11:07:36,960 >> loading file tokenizer.model from cache at /home/jupyter/datasphere/project/modelcache/models--haoranxu--X-ALMA-13B-Pretrain/snapshots/c57c245bbd2a9c165ea1da6c24f857d8107afcbf/tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-10-11 11:07:36,961 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-10-11 11:07:36,961 >> loading file special_tokens_map.json from cache at /home/jupyter/datasphere/project/modelcache/models--haoranxu--X-ALMA-13B-Pretrain/snapshots/c57c245bbd2a9c165ea1da6c24f857d8107afcbf/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-10-11 11:07:36,962 >> loading file tokenizer_config.json from cache at /home/jupyter/datasphere/project/modelcache/models--haoranxu--X-ALMA-13B-Pretrain/snapshots/c57c245bbd2a9c165ea1da6c24f857d8107afcbf/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-10-11 11:07:36,962 >> loading file tokenizer.json from cache at /home/jupyter/datasphere/project/modelcache/models--haoranxu--X-ALMA-13B-Pretrain/snapshots/c57c245bbd2a9c165ea1da6c24f857d8107afcbf/tokenizer.json\n",
      "Running CPO preprocessing:   0%|          | 0/2009 [00:00<?, ? examples/s]Caching processed dataset at /home/jupyter/datasphere/project/datasetscache/marulyanova___alma-r-preference-f/default-31a8eb8e26a3d2e8/0.0.0/e0b54ddc5efda729a00ee89c8215eb1ed5e14423/cache-7e2c1fe8be0206ad.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/11/2024 11:07:37 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/jupyter/datasphere/project/datasetscache/marulyanova___alma-r-preference-f/default-31a8eb8e26a3d2e8/0.0.0/e0b54ddc5efda729a00ee89c8215eb1ed5e14423/cache-7e2c1fe8be0206ad.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running CPO preprocessing: 100%|██████████| 2009/2009 [00:01<00:00, 1265.21 examples/s]\n",
      "Caching indices mapping at /home/jupyter/datasphere/project/datasetscache/marulyanova___alma-r-preference-f/default-31a8eb8e26a3d2e8/0.0.0/e0b54ddc5efda729a00ee89c8215eb1ed5e14423/cache-2b8a52fd1455db45.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/11/2024 11:07:38 - INFO - datasets.arrow_dataset - Caching indices mapping at /home/jupyter/datasphere/project/datasetscache/marulyanova___alma-r-preference-f/default-31a8eb8e26a3d2e8/0.0.0/e0b54ddc5efda729a00ee89c8215eb1ed5e14423/cache-2b8a52fd1455db45.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:673] 2024-10-11 11:07:38,711 >> loading configuration file ALMA/configs/conf_axinia.json\n",
      "[INFO|configuration_utils.py:742] 2024-10-11 11:07:38,713 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"ALMA/configs/conf_axinia.json\",\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"compute_environment\": \"LOCAL_MACHINE\",\n",
      "  \"deepspeed_config\": {\n",
      "    \"gradient_accumulation_steps\": 1,\n",
      "    \"gradient_clipping\": 1.0,\n",
      "    \"offload_optimizer_device\": \"cuda\",\n",
      "    \"offload_param_device\": \"cpu\",\n",
      "    \"zero3_init_flag\": false,\n",
      "    \"zero_stage\": 2\n",
      "  },\n",
      "  \"distributed_type\": \"DEEPSPEED\",\n",
      "  \"downcast_bf16\": \"no\",\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"machine_rank\": 0,\n",
      "  \"main_training_function\": \"main\",\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"mixed_precision\": \"bf16\",\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"num_machines\": 1,\n",
      "  \"num_processes\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rdzv_backend\": \"static\",\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"same_network\": true,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tpu_env\": [],\n",
      "  \"tpu_use_cluster\": false,\n",
      "  \"tpu_use_sudo\": false,\n",
      "  \"transformers_version\": \"4.45.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_cpu\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3732] 2024-10-11 11:07:39,361 >> loading weights file model.safetensors from cache at /home/jupyter/datasphere/project/modelcache/models--haoranxu--X-ALMA-13B-Pretrain/snapshots/c57c245bbd2a9c165ea1da6c24f857d8107afcbf/model.safetensors.index.json\n",
      "Downloading shards: 100%|██████████| 6/6 [00:00<00:00,  6.05it/s]\n",
      "[INFO|configuration_utils.py:1099] 2024-10-11 11:07:40,357 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 512\n",
      "}\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Trying to set a tensor of shape torch.Size([32000, 5120]) in \"weight\" (which has shape torch.Size([32000, 4096])), this looks incorrect.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/work/resources/ALMA/run_cpo_llmmt.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/work/resources/ALMA/run_cpo_llmmt.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;31m# Load model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;31m# Initialize our Trainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/resources/ALMA/utils/utils.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(data_args, model_args, training_args, tokenizer, logger)\u001b[0m\n\u001b[1;32m    505\u001b[0m             )\n\u001b[1;32m    506\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m             model = AutoModelLoad.from_pretrained(\n\u001b[0m\u001b[1;32m    508\u001b[0m                 \u001b[0mmodel_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlast_checkpoint\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlast_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m                 \u001b[0mfrom_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".ckpt\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    565\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             )\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4012\u001b[0m                 \u001b[0moffload_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4013\u001b[0m                 \u001b[0merror_msgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4014\u001b[0;31m             \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_pretrained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4015\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4016\u001b[0m                 \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path)\u001b[0m\n\u001b[1;32m   4500\u001b[0m                                 )\n\u001b[1;32m   4501\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4502\u001b[0;31m                         new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(\n\u001b[0m\u001b[1;32m   4503\u001b[0m                             \u001b[0mmodel_to_load\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4504\u001b[0m                             \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys, pretrained_model_name_or_path)\u001b[0m\n\u001b[1;32m    971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0;31m# For backward compatibility with older versions of `accelerate` and for non-quantized params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m             \u001b[0mset_module_tensor_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mset_module_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m             \u001b[0mhf_quantizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_quantized_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/utils/modeling.py\u001b[0m in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;31m# In other cases, we want to make sure we're not loading checkpoints that do not match the config.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mold_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparam_cls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"Params4bit\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    287\u001b[0m                 \u001b[0;34mf'Trying to set a tensor of shape {value.shape} in \"{tensor_name}\" (which has shape {old_value.shape}), this looks incorrect.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             )\n",
      "\u001b[0;31mValueError\u001b[0m: Trying to set a tensor of shape torch.Size([32000, 5120]) in \"weight\" (which has shape torch.Size([32000, 4096])), this looks incorrect."
     ]
    }
   ],
   "source": [
    "%run ./ALMA/run_cpo_llmmt.py \\\n",
    "    --config_name ALMA/configs/conf_axinia.json \\\n",
    "    --model_name_or_path haoranxu/X-ALMA-13B-Pretrain \\\n",
    "    --tokenizer_name haoranxu/X-ALMA-13B-Pretrain \\\n",
    "    --peft_model_id  haoranxu/ALMA-13B-Pretrain-LoRA \\\n",
    "    --cpo_scorer kiwi_xcomet \\\n",
    "    --beta 0.1 \\\n",
    "    --use_peft \\\n",
    "    --use_fast_tokenizer False \\\n",
    "    --cpo_data_path  marulyanova/ALMA-R-Preference-F \\\n",
    "    --do_train \\\n",
    "    --language_pairs \"['ru-en']\" \\\n",
    "    --low_cpu_mem_usage \\\n",
    "    --bf16 \\\n",
    "    --learning_rate 1e-4 \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --gradient_accumulation_steps 1 \\\n",
    "    --lr_scheduler_type inverse_sqrt \\\n",
    "    --warmup_ratio 0.01 \\\n",
    "    --ignore_pad_token_for_loss \\\n",
    "    --ignore_prompt_token_for_loss \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --evaluation_strategy no \\\n",
    "    --save_strategy steps \\\n",
    "    --save_total_limit 1 \\\n",
    "    --logging_strategy steps \\\n",
    "    --logging_steps 0.05 \\\n",
    "    --output_dir dist \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --prediction_loss_only \\\n",
    "    --max_new_tokens 256 \\\n",
    "    --max_source_length 256 \\\n",
    "    --max_prompt_length 256 \\\n",
    "    --max_length 512 \\\n",
    "    --seed 42 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --report_to none \\\n",
    "    --overwrite_cache \\\n",
    "    --eval_accumulation_steps 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d87414-10e0-4c7c-ae67-dbf795e047bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
