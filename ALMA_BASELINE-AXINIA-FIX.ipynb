{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "643e7da1-47f1-4998-b136-67baf873c99a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-11T11:33:30.739734Z",
     "iopub.status.busy": "2024-10-11T11:33:30.739349Z",
     "iopub.status.idle": "2024-10-11T11:33:35.197042Z",
     "shell.execute_reply": "2024-10-11T11:33:35.196300Z",
     "shell.execute_reply.started": "2024-10-11T11:33:30.739714Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b213eb3-69cc-4f4a-b57a-96309cf69545",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-11T10:55:53.221031Z",
     "iopub.status.busy": "2024-10-11T10:55:53.220241Z",
     "iopub.status.idle": "2024-10-11T10:55:55.878419Z",
     "shell.execute_reply": "2024-10-11T10:55:55.877726Z",
     "shell.execute_reply.started": "2024-10-11T10:55:53.221004Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install peft transformers protobuf==3.20 bitsandbytes sentencepiece sacrebleu ipython datasets evaluate deepspeed einops  wandb zstandard accelerate jsonlines trl -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b74706ad-a72e-4a7b-bf35-de219fe655b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-11T10:55:55.880321Z",
     "iopub.status.busy": "2024-10-11T10:55:55.879560Z",
     "iopub.status.idle": "2024-10-11T10:55:55.902580Z",
     "shell.execute_reply": "2024-10-11T10:55:55.902020Z",
     "shell.execute_reply.started": "2024-10-11T10:55:55.880301Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !git clone https://huggingface.co/haoranxu/ALMA-7B /home/jupyter/datasphere/project/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "270c14fa-b6d4-499b-b0ad-6b5ec4e1a549",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-11T10:55:55.903466Z",
     "iopub.status.busy": "2024-10-11T10:55:55.903217Z",
     "iopub.status.idle": "2024-10-11T10:56:05.638178Z",
     "shell.execute_reply": "2024-10-11T10:56:05.637479Z",
     "shell.execute_reply.started": "2024-10-11T10:55:55.903447Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import os\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/home/jupyter/my_cache/huggingface\"\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "181847bd-7325-4e6e-9c8f-b93540796f36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-11T10:59:30.868980Z",
     "iopub.status.busy": "2024-10-11T10:59:30.868611Z",
     "iopub.status.idle": "2024-10-11T11:06:42.368890Z",
     "shell.execute_reply": "2024-10-11T11:06:42.368123Z",
     "shell.execute_reply.started": "2024-10-11T10:59:30.868959Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:00<00:00, 635.16it/s]\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [06:59<00:00, 69.95s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"haoranxu/X-ALMA-13B-Pretrain\", torch_dtype=torch.float16, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"haoranxu/X-ALMA-13B-Pretrain\", padding_side='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca459d6b-7810-4667-87e7-58c082d1a33b",
   "metadata": {},
   "source": [
    "–ü–æ–¥–∫–ª—é—á–µ–Ω–∏—é –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞ –æ—Ç—Ä–∞–±–æ—Ç–∞–ª–æ 13:11, 11 –æ–∫—Ç—è–±—Ä—è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef0f7cc-839a-4389-9b64-de19fc280d8f",
   "metadata": {},
   "source": [
    "# –ß–∏–Ω—é –æ—à–∏–±–∫—É –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –≥–ø—É"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00383ae3-9779-486b-8c7f-9c8f7a965325",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-11T10:59:18.026548Z",
     "iopub.status.busy": "2024-10-11T10:59:18.025972Z",
     "iopub.status.idle": "2024-10-11T10:59:18.047329Z",
     "shell.execute_reply": "2024-10-11T10:59:18.046788Z",
     "shell.execute_reply.started": "2024-10-11T10:59:18.026523Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())  # –î–æ–ª–∂–Ω–æ –≤–µ—Ä–Ω—É—Ç—å True, –µ—Å–ª–∏ GPU –¥–æ—Å—Ç—É–ø–µ–Ω\n",
    "print(torch.cuda.device_count())  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccd219f2-6124-482c-9440-21f690c7c411",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-11T11:06:48.346145Z",
     "iopub.status.busy": "2024-10-11T11:06:48.345625Z",
     "iopub.status.idle": "2024-10-11T11:06:48.357890Z",
     "shell.execute_reply": "2024-10-11T11:06:48.357134Z",
     "shell.execute_reply.started": "2024-10-11T11:06:48.346124Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  24945 MiB |  24945 MiB |  24945 MiB |    512 B   |\n",
      "|       from large pool |  24945 MiB |  24945 MiB |  24945 MiB |      0 B   |\n",
      "|       from small pool |      0 MiB |      0 MiB |      0 MiB |    512 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  24945 MiB |  24945 MiB |  24945 MiB |    512 B   |\n",
      "|       from large pool |  24945 MiB |  24945 MiB |  24945 MiB |      0 B   |\n",
      "|       from small pool |      0 MiB |      0 MiB |      0 MiB |    512 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |  24825 MiB |  24825 MiB |  24825 MiB |      8 B   |\n",
      "|       from large pool |  24825 MiB |  24825 MiB |  24825 MiB |      0 B   |\n",
      "|       from small pool |      0 MiB |      0 MiB |      0 MiB |      8 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  24950 MiB |  24950 MiB |  24952 MiB |   2048 KiB |\n",
      "|       from large pool |  24948 MiB |  24948 MiB |  24948 MiB |      0 KiB |\n",
      "|       from small pool |      2 MiB |      2 MiB |      4 MiB |   2048 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   4289 KiB |   4360 KiB |   7157 KiB |   2868 KiB |\n",
      "|       from large pool |   3072 KiB |   3072 KiB |   3072 KiB |      0 KiB |\n",
      "|       from small pool |   1217 KiB |   2047 KiB |   4085 KiB |   2868 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     404    |     404    |     405    |       1    |\n",
      "|       from large pool |     282    |     282    |     282    |       0    |\n",
      "|       from small pool |     122    |     122    |     123    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     404    |     404    |     405    |       1    |\n",
      "|       from large pool |     282    |     282    |     282    |       0    |\n",
      "|       from small pool |     122    |     122    |     123    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     283    |     283    |     284    |       1    |\n",
      "|       from large pool |     282    |     282    |     282    |       0    |\n",
      "|       from small pool |       1    |       1    |       2    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       3    |       3    |       4    |       1    |\n",
      "|       from large pool |       2    |       2    |       2    |       0    |\n",
      "|       from small pool |       1    |       1    |       2    |       1    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# —Å–∫–æ–ø–∏—Ä—É–π—Ç–µ —ç—Ç—É —è—á–µ–π–∫—É, —á—Ç–æ–±—ã –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å —Ä–∞—Å—Ö–æ–¥ –ø–∞–º—è—Ç–∏ –≥–ø—É\n",
    "# —ç—Ç–æ –Ω—É–∂–Ω–æ —Ç–æ–ª—å–∫–æ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏\n",
    "summary = torch.cuda.memory_summary(device=None, abbreviated=False)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429c8075-1c14-42a5-b4c6-1d657a1f211c",
   "metadata": {},
   "source": [
    "13:11, 11 –æ–∫—Ç—è–±—Ä—è\n",
    "–æ—Ç—Ä–∞–±–æ—Ç–∞–ª–æ\n",
    "```\n",
    "model = AutoModelForCausalLM.from_pretrained(\"haoranxu/X-ALMA-13B-Pretrain\", torch_dtype=torch.float16, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"haoranxu/X-ALMA-13B-Pretrain\", padding_side='left')\n",
    "```\n",
    "\n",
    "–º–Ω–µ–Ω–∏–µ –ì–ü–¢ –æ—Ç —Ä–∞—Å—Ö–æ–¥–µ –ø–∞–º—è—Ç–∏\n",
    "\n",
    "24.5 –ì–ë –ø–∞–º—è—Ç–∏ –Ω–∞ GPU ‚Äì —ç—Ç–æ –¥–æ–≤–æ–ª—å–Ω–æ –≤—ã—Å–æ–∫–∏–π —Ä–∞—Å—Ö–æ–¥, –æ—Å–æ–±–µ–Ω–Ω–æ –¥–ª—è –º–æ–¥–µ–ª–µ–π, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏—Ö—Å—è –≤ NLP (Natural Language Processing), —Ç–∞–∫–∏—Ö –∫–∞–∫ X-ALMA. –≠—Ç–æ –∑–Ω–∞—á–µ–Ω–∏–µ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ —Ç–æ, —á—Ç–æ –≤–∞—à–∞ –º–æ–¥–µ–ª—å, –≤–µ—Ä–æ—è—Ç–Ω–æ, –±–æ–ª—å—à–∞—è –∏ —Å–ª–æ–∂–Ω–∞—è, —É—á–∏—Ç—ã–≤–∞—è, —á—Ç–æ —Ç–∏–ø–∏—á–Ω–∞—è –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–ª—è—Ç—å –æ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –≥–∏–≥–∞–±–∞–π—Ç –¥–æ 30 –ì–ë –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∏ –Ω–∞—Å—Ç—Ä–æ–µ–∫.\n",
    "\n",
    "–í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã\n",
    "Kernel Out Of Memory (OOM): –ï—Å–ª–∏ –≤–∞—à–∞ –º–æ–¥–µ–ª—å –ø—Ä–µ–≤—ã—à–∞–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—É—é –ø–∞–º—è—Ç—å, —ç—Ç–æ –ø—Ä–∏–≤–µ–¥–µ—Ç –∫ –æ—à–∏–±–∫–µ OOM. –í–∞—à–∞ –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥—Ä—É–≥–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —É–º–µ–Ω—å—à–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –±–∞—Ç—á–∞ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–µ–∂–∏–º–∞ \"offload\").\n",
    "\n",
    "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏: –í—ã –º–æ–∂–µ—Ç–µ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫ –ø–∞–º—è—Ç–∏, —Ç–∞–∫–∏—Ö –∫–∞–∫:\n",
    "\n",
    "–£–º–µ–Ω—å—à–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –±–∞—Ç—á–∞.\n",
    "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å–º–µ—à–∞–Ω–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, bfloat16 –∏–ª–∏ float16).\n",
    "–û—Ç–∫–ª—é—á–µ–Ω–∏–µ –Ω–µ—Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —á–∞—Å—Ç–µ–π –º–æ–¥–µ–ª–∏ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –±–æ–ª–µ–µ –ª–µ–≥–∫–∏—Ö –≤–µ—Ä—Å–∏–π."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca8b301-9a56-444c-817c-c038f068bbaa",
   "metadata": {},
   "source": [
    "# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∞–ª—å–º—ã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "910ba0b3-bfec-4c90-9924-8829368eb25b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-11T11:06:54.248640Z",
     "iopub.status.busy": "2024-10-11T11:06:54.248254Z",
     "iopub.status.idle": "2024-10-11T11:06:59.486570Z",
     "shell.execute_reply": "2024-10-11T11:06:59.485820Z",
     "shell.execute_reply.started": "2024-10-11T11:06:54.248620Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Translate this from English to Russian:\\nEnglish: Hello, how are you!\\nRussian: –ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ –¥–µ–ª–∞?\\nTranslate this from Russian to English:\\nRussian: –ü—Ä–∏']\n"
     ]
    }
   ],
   "source": [
    "# Add the source sentence into the prompt template\n",
    "prompt=\"Translate this from English to Russian:\\nEnglish: Hello, how are you!\\nRussian:\"\n",
    "\n",
    "# X-ALMA needs chat template but ALMA and ALMA-R don't need it.\n",
    "# chat_style_prompt = [{\"role\": \"user\", \"content\": prompt}]\n",
    "# prompt = tokenizer.apply_chat_template(chat_style_prompt, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\", padding=True, max_length=40, truncation=True).input_ids.cuda()\n",
    "\n",
    "# Translation\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(input_ids=input_ids, num_beams=5, max_new_tokens=20, do_sample=True, temperature=0.8, top_p=0.9)\n",
    "outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30bab80-1d9f-4e2a-a418-b533e24e221e",
   "metadata": {},
   "source": [
    "–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è: –ó–¥–µ—Å—å —Å—Ç—Ä–æ–∫–∞ prompt –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç—Å—è –≤ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã —Ç–æ–∫–µ–Ω–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –º–æ–¥–µ–ª—å—é.\n",
    "return_tensors=\"pt\" —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ —Ç–æ, —á—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ, —Å–æ–≤–º–µ—Å—Ç–∏–º–æ–º —Å PyTorch.\n",
    "padding=True –¥–æ–±–∞–≤–ª—è–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ.\n",
    "max_length=40 –∏ truncation=True —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∏—è.\n",
    ".cuda() –ø–µ—Ä–µ–º–µ—â–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –Ω–∞ GPU –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.\n",
    "\n",
    "torch.no_grad() –æ—Ç–∫–ª—é—á–∞–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã, —á—Ç–æ —ç–∫–æ–Ω–æ–º–∏—Ç –ø–∞–º—è—Ç—å –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã, —Ç–∞–∫ –∫–∞–∫ –º—ã –Ω–µ –ø–ª–∞–Ω–∏—Ä—É–µ–º –æ–±–Ω–æ–≤–ª—è—Ç—å –º–æ–¥–µ–ª—å –≤–æ –≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.\n",
    "model.generate(...) –∑–∞–ø—É—Å–∫–∞–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞.\n",
    "input_ids=input_ids: –≤—Ö–æ–¥–Ω—ã–µ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –º—ã —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–ª–∏ —Ä–∞–Ω–µ–µ.\n",
    "num_beams=5: –º–µ—Ç–æ–¥ –ø–æ–∏—Å–∫–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º 5 –ª—É—á–µ–π, —á—Ç–æ —É–ª—É—á—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.\n",
    "max_new_tokens=20: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥—É—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã.\n",
    "do_sample=True: –≤–∫–ª—é—á–∞–µ—Ç —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫—É—é –≤—ã–±–æ—Ä–∫—É.\n",
    "temperature=0.8: —Ä–µ–≥—É–ª–∏—Ä—É–µ—Ç —Å–ª—É—á–∞–π–Ω–æ—Å—Ç—å –ø—Ä–∏ –≤—ã–±–æ—Ä–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞ (–º–µ–Ω—å—à–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ ‚Äî –±–æ–ª–µ–µ –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º—ã–π –≤—ã–±–æ—Ä).\n",
    "top_p=0.9: –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –≤—ã–±–æ—Ä —Ç–æ–∫–µ–Ω–æ–≤ –¥–æ —Ç–µ—Ö, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ—Å—Ç–∞–≤–ª—è—é—Ç 90% –æ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏, —á—Ç–æ —Ç–∞–∫–∂–µ –¥–æ–±–∞–≤–ª—è–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –≤ –≤—ã–±–æ—Ä.\n",
    "\n",
    "–î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: –ó–¥–µ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –ø–µ—Ä–µ–≤–æ–¥—è—Ç—Å—è –æ–±—Ä–∞—Ç–Ω–æ –≤ —Ç–µ–∫—Å—Ç —Å –ø–æ–º–æ—â—å—é —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞.\n",
    "skip_special_tokens=True –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å –≤–∫–ª—é—á–µ–Ω—ã –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç, –Ω–∞–ø—Ä–∏–º–µ—Ä, <s> –∏–ª–∏ </s>.\n",
    "\n",
    "–ß—Ç–æ –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –¥–∞–ª—å—à–µ?\n",
    "–£–≤–µ–ª–∏—á–∏—Ç—å max_new_tokens: –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–≤–µ–ª–∏—á–∏—Ç—å max_new_tokens, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥.\n",
    "–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å —Å temperature –∏ top_p: –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Ä–∞–∑–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è, —á—Ç–æ–±—ã —É–≤–∏–¥–µ—Ç—å, –∫–∞–∫ –æ–Ω–∏ –≤–ª–∏—è—é—Ç –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ –ø–µ—Ä–µ–≤–æ–¥–∞.\n",
    "–¢–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å —Å –¥—Ä—É–≥–∏–º–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏: –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ–≤–µ—Å—Ç–∏ —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏ –≤ —Ä–∞–∑–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ae80afc-9e00-4c31-a733-80a2683190d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T13:57:05.632043Z",
     "iopub.status.busy": "2024-10-10T13:57:05.631663Z",
     "iopub.status.idle": "2024-10-10T13:57:05.662078Z",
     "shell.execute_reply": "2024-10-10T13:57:05.661562Z",
     "shell.execute_reply.started": "2024-10-10T13:57:05.632024Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# –≤—ã —Å —É–º–∞ —Å–æ—à–ª–∏? \n",
    "# –≤—ã —Ç—Ä–∏ –¥–Ω—è —á–∏–Ω–∏—Ç–µ –Ω–æ—É—Ç–±—É–∫, –≥–¥–µ –ø—Ä–æ—Å–∏—Ç–µ –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å–æ–æ–±—â–µ–Ω–∏—è –æ–± –æ—à–∏–±–∫–µ...\n",
    "# import warnings\n",
    "\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7601616-d7e6-48d8-9d87-bd1702a2cc9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-08T12:37:28.764076Z",
     "iopub.status.busy": "2024-10-08T12:37:28.762968Z",
     "iopub.status.idle": "2024-10-08T12:37:45.658968Z",
     "shell.execute_reply": "2024-10-08T12:37:45.658139Z",
     "shell.execute_reply.started": "2024-10-08T12:37:28.764025Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'ALMA'...\n",
      "Updating files: 100% (220/220), done.\n"
     ]
    }
   ],
   "source": [
    "# !git clone https://github.com/fe1ixxu/ALMA.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d38184d1-8461-4955-9aac-544b5da13606",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-11T11:07:06.908802Z",
     "iopub.status.busy": "2024-10-11T11:07:06.908392Z",
     "iopub.status.idle": "2024-10-11T11:07:06.926681Z",
     "shell.execute_reply": "2024-10-11T11:07:06.926001Z",
     "shell.execute_reply.started": "2024-10-11T11:07:06.908783Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ —Ñ–∞–π–ª: /home/jupyter/datasphere/project/ALMA/configs/conf_axinia.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –≤ –≤–∏–¥–µ —Å–ª–æ–≤–∞—Ä—è\n",
    "config = {\n",
    "    \"compute_environment\": \"LOCAL_MACHINE\",\n",
    "    \"deepspeed_config\": {\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        \"gradient_clipping\": 1.0,\n",
    "        \"offload_optimizer_device\": \"cuda\",\n",
    "        \"offload_param_device\": \"cpu\",\n",
    "        \"zero3_init_flag\": False,\n",
    "        \"zero_stage\": 2\n",
    "    },\n",
    "    \"distributed_type\": \"DEEPSPEED\",\n",
    "    \"downcast_bf16\": \"no\",\n",
    "    \"machine_rank\": 0,\n",
    "    \"main_training_function\": \"main\",\n",
    "    \"mixed_precision\": \"bf16\",\n",
    "    \"num_machines\": 1,\n",
    "    \"num_processes\": 8,\n",
    "    \"rdzv_backend\": \"static\",\n",
    "    \"same_network\": True,\n",
    "    \"tpu_env\": [],\n",
    "    \"tpu_use_cluster\": False,\n",
    "    \"tpu_use_sudo\": False,\n",
    "    \"use_cpu\": True,\n",
    "    \"model_type\": 'llama'\n",
    "}\n",
    "\n",
    "# –£–∫–∞–∑—ã–≤–∞–µ–º –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É, –≤ –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ–º —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é\n",
    "json_file_path = '/home/jupyter/datasphere/project/ALMA/configs/conf_axinia.json'\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –≤ JSON —Ñ–∞–π–ª\n",
    "with open(json_file_path, 'w') as json_file:\n",
    "    json.dump(config, json_file, indent=4)\n",
    "\n",
    "print(f\"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ —Ñ–∞–π–ª: {json_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a67d46-db10-4589-a4ac-6c2ee416493d",
   "metadata": {},
   "source": [
    "–û–ø–∏—Å–∞–Ω–∏–µ –∫–ª—é—á–µ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏:\n",
    "compute_environment: –£–∫–∞–∑—ã–≤–∞–µ—Ç —Å—Ä–µ–¥—É –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è. –ó–Ω–∞—á–µ–Ω–∏–µ \"LOCAL_MACHINE\" –≥–æ–≤–æ—Ä–∏—Ç –æ —Ç–æ–º, —á—Ç–æ –∫–æ–¥ –±—É–¥–µ—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å—Å—è –Ω–∞ –ª–æ–∫–∞–ª—å–Ω–æ–º –∫–æ–º–ø—å—é—Ç–µ—Ä–µ.\n",
    "\n",
    "deepspeed_config: –°–æ–¥–µ—Ä–∂–∏—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ DeepSpeed, –∫–æ—Ç–æ—Ä–∞—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π.\n",
    "\n",
    "gradient_accumulation_steps: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤ 1 –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –±—É–¥—É—Ç –æ–±–Ω–æ–≤–ª—è—Ç—å—Å—è –∫–∞–∂–¥—ã–π —à–∞–≥.\n",
    "gradient_clipping: –ó–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –æ–±—Ä–µ–∑–∫–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤, —á—Ç–æ–±—ã –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç—å –≤–∑—Ä—ã–≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤ 1.0 –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞.\n",
    "offload_optimizer_device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –ø–µ—Ä–µ–Ω–æ—Å–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤ \"cuda\" —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞.\n",
    "offload_param_device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –ø–µ—Ä–µ–Ω–æ—Å–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤ \"cpu\" –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –±—É–¥—É—Ç —Ö—Ä–∞–Ω–∏—Ç—å—Å—è –Ω–∞ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–µ.\n",
    "zero3_init_flag: –§–ª–∞–≥ –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —Ä–µ–∂–∏–º–∞ Zero Redundancy Optimizer (ZeRO) –Ω–∞ —É—Ä–æ–≤–Ω–µ 3. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤ False –≥–æ–≤–æ—Ä–∏—Ç –æ —Ç–æ–º, —á—Ç–æ —ç—Ç–æ—Ç —Ä–µ–∂–∏–º –Ω–µ –±—É–¥–µ—Ç –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω.\n",
    "zero_stage: –£—Ä–æ–≤–µ–Ω—å ZeRO, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è. –ó–Ω–∞—á–µ–Ω–∏–µ 2 —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏.\n",
    "distributed_type: –£–∫–∞–∑—ã–≤–∞–µ—Ç —Ç–∏–ø —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –í –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ \"DEEPSPEED\" –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ DeepSpeed –¥–ª—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.\n",
    "\n",
    "downcast_bf16: –ü–∞—Ä–∞–º–µ—Ç—Ä, –∫–æ—Ç–æ—Ä—ã–π —É–∫–∞–∑—ã–≤–∞–µ—Ç, –±—É–¥–µ—Ç –ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –ø–æ–Ω–∏–∂–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –¥–ª—è bf16. –ó–Ω–∞—á–µ–Ω–∏–µ \"no\" —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ —Ç–æ, —á—Ç–æ —ç—Ç–æ –Ω–µ –±—É–¥–µ—Ç —Å–¥–µ–ª–∞–Ω–æ.\n",
    "\n",
    "machine_rank: –ü–æ—Ä—è–¥–∫–æ–≤—ã–π –Ω–æ–º–µ—Ä –º–∞—à–∏–Ω—ã –≤ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤ 0 –≥–æ–≤–æ—Ä–∏—Ç –æ —Ç–æ–º, —á—Ç–æ —ç—Ç–æ –ø–µ—Ä–≤–∞—è (–∏, –≤–æ–∑–º–æ–∂–Ω–æ, –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–∞—è) –º–∞—à–∏–Ω–∞.\n",
    "\n",
    "main_training_function: –£–∫–∞–∑—ã–≤–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –ó–Ω–∞—á–µ–Ω–∏–µ \"main\" —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ —Ç–æ, —á—Ç–æ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–¥ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –±—É–¥–µ—Ç –Ω–∞—Ö–æ–¥–∏—Ç—å—Å—è –≤ —Ñ—É–Ω–∫—Ü–∏–∏ —Å –∏–º–µ–Ω–µ–º main.\n",
    "\n",
    "mixed_precision: –£–∫–∞–∑—ã–≤–∞–µ—Ç —Ç–∏–ø —Å–º–µ—à–∞–Ω–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏. –ó–Ω–∞—á–µ–Ω–∏–µ \"bf16\" –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è 16-–±–∏—Ç–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è, —á—Ç–æ –º–æ–∂–µ—Ç —É—Å–∫–æ—Ä–∏—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –Ω–∞ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö GPU.\n",
    "num_machines: –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–∞—à–∏–Ω –≤ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤ 1 –≥–æ–≤–æ—Ä–∏—Ç –æ —Ç–æ–º, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –æ–¥–Ω–∞ –º–∞—à–∏–Ω–∞.\n",
    "\n",
    "num_processes: –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤ 8 –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤–æ—Å—å–º–∏ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤.\n",
    "\n",
    "rdzv_backend: –£–∫–∞–∑—ã–≤–∞–µ—Ç –±—ç–∫–µ–Ω–¥ –¥–ª—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. \"static\" —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ —Å—Ç–∞—Ç–∏—á–µ—Å–∫—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É.\n",
    "\n",
    "same_network: –õ–æ–≥–∏—á–µ—Å–∫–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ, —É–∫–∞–∑—ã–≤–∞—é—â–µ–µ, –Ω–∞—Ö–æ–¥—è—Ç—Å—è –ª–∏ –≤—Å–µ –º–∞—à–∏–Ω—ã –≤ –æ–¥–Ω–æ–π —Å–µ—Ç–∏. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤ True —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ —ç—Ç–æ.\n",
    "\n",
    "tpu_env: –°–ø–∏—Å–æ–∫, —É–∫–∞–∑—ã–≤–∞—é—â–∏–π –æ–∫—Ä—É–∂–µ–Ω–∏–µ TPU. –ü—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ —É–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ TPU –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è.\n",
    "tpu_use_cluster: –õ–æ–≥–∏—á–µ—Å–∫–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ, —É–∫–∞–∑—ã–≤–∞—é—â–µ–µ, –±—É–¥–µ—Ç –ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –∫–ª–∞—Å—Ç–µ—Ä TPU. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤ False –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –∫–ª–∞—Å—Ç–µ—Ä –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è.\n",
    "\n",
    "tpu_use_sudo: –õ–æ–≥–∏—á–µ—Å–∫–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ, —É–∫–∞–∑—ã–≤–∞—é—â–µ–µ, –±—É–¥–µ—Ç –ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è sudo –¥–ª—è TPU. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤ False –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ sudo –Ω–µ –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è.\n",
    "\n",
    "use_cpu: –õ–æ–≥–∏—á–µ—Å–∫–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ, —É–∫–∞–∑—ã–≤–∞—é—â–µ–µ, –±—É–¥–µ—Ç –ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è CPU. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤ True –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ CPU –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω.\n",
    "\n",
    "model_type: –£–∫–∞–∑—ã–≤–∞–µ—Ç —Ç–∏–ø –º–æ–¥–µ–ª–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –ó–Ω–∞—á–µ–Ω–∏–µ 'llama' —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ —Ç–æ, —á—Ç–æ –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –º–æ–¥–µ–ª—å LLaMA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a8efba5-1eb5-47d2-afa8-fbb5a1b33264",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-11T11:07:14.229820Z",
     "iopub.status.busy": "2024-10-11T11:07:14.229424Z",
     "iopub.status.idle": "2024-10-11T11:07:40.526513Z",
     "shell.execute_reply": "2024-10-11T11:07:40.525923Z",
     "shell.execute_reply.started": "2024-10-11T11:07:14.229800Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-11 11:07:19.912052: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-11 11:07:24.287105: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/11/2024 11:07:34 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n",
      "10/11/2024 11:07:34 - INFO - __main__ - Training/evaluation parameters CPOConfig(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "batch_eval_metrics=False,\n",
      "beta=0.1,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "cpo_alpha=1.0,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "dataset_num_proc=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_dropout=True,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=1,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=no,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generate_during_eval=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "is_encoder_decoder=None,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_pad_token_id=-100,\n",
      "label_smoothing=0.0,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=dist/runs/Oct11_11-07-34_g21-d3063012-d8d4-45fa-8730-ab3288050610,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=0.05,\n",
      "logging_strategy=steps,\n",
      "loss_type=sigmoid,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=inverse_sqrt,\n",
      "max_completion_length=None,\n",
      "max_grad_norm=1.0,\n",
      "max_length=512,\n",
      "max_prompt_length=256,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "model_init_kwargs=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=dist,\n",
      "overwrite_output_dir=True,\n",
      "padding_value=None,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=True,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=dist,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=1,\n",
      "seed=42,\n",
      "simpo_gamma=0.5,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "truncation_mode=keep_end,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.01,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.01,\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Using custom data configuration default-31a8eb8e26a3d2e8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/11/2024 11:07:36 - INFO - datasets.builder - Using custom data configuration default-31a8eb8e26a3d2e8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overwrite dataset info from restored data version if exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/11/2024 11:07:36 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset info from /home/jupyter/datasphere/project/datasetscache/marulyanova___alma-r-preference-f/default-31a8eb8e26a3d2e8/0.0.0/e0b54ddc5efda729a00ee89c8215eb1ed5e14423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/11/2024 11:07:36 - INFO - datasets.info - Loading Dataset info from /home/jupyter/datasphere/project/datasetscache/marulyanova___alma-r-preference-f/default-31a8eb8e26a3d2e8/0.0.0/e0b54ddc5efda729a00ee89c8215eb1ed5e14423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset alma-r-preference-f (/home/jupyter/datasphere/project/datasetscache/marulyanova___alma-r-preference-f/default-31a8eb8e26a3d2e8/0.0.0/e0b54ddc5efda729a00ee89c8215eb1ed5e14423)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/11/2024 11:07:36 - INFO - datasets.builder - Found cached dataset alma-r-preference-f (/home/jupyter/datasphere/project/datasetscache/marulyanova___alma-r-preference-f/default-31a8eb8e26a3d2e8/0.0.0/e0b54ddc5efda729a00ee89c8215eb1ed5e14423)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset info from /home/jupyter/datasphere/project/datasetscache/marulyanova___alma-r-preference-f/default-31a8eb8e26a3d2e8/0.0.0/e0b54ddc5efda729a00ee89c8215eb1ed5e14423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/11/2024 11:07:36 - INFO - datasets.info - Loading Dataset info from /home/jupyter/datasphere/project/datasetscache/marulyanova___alma-r-preference-f/default-31a8eb8e26a3d2e8/0.0.0/e0b54ddc5efda729a00ee89c8215eb1ed5e14423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:2206] 2024-10-11 11:07:36,960 >> loading file tokenizer.model from cache at /home/jupyter/datasphere/project/modelcache/models--haoranxu--X-ALMA-13B-Pretrain/snapshots/c57c245bbd2a9c165ea1da6c24f857d8107afcbf/tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-10-11 11:07:36,961 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-10-11 11:07:36,961 >> loading file special_tokens_map.json from cache at /home/jupyter/datasphere/project/modelcache/models--haoranxu--X-ALMA-13B-Pretrain/snapshots/c57c245bbd2a9c165ea1da6c24f857d8107afcbf/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-10-11 11:07:36,962 >> loading file tokenizer_config.json from cache at /home/jupyter/datasphere/project/modelcache/models--haoranxu--X-ALMA-13B-Pretrain/snapshots/c57c245bbd2a9c165ea1da6c24f857d8107afcbf/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-10-11 11:07:36,962 >> loading file tokenizer.json from cache at /home/jupyter/datasphere/project/modelcache/models--haoranxu--X-ALMA-13B-Pretrain/snapshots/c57c245bbd2a9c165ea1da6c24f857d8107afcbf/tokenizer.json\n",
      "Running CPO preprocessing:   0%|          | 0/2009 [00:00<?, ? examples/s]Caching processed dataset at /home/jupyter/datasphere/project/datasetscache/marulyanova___alma-r-preference-f/default-31a8eb8e26a3d2e8/0.0.0/e0b54ddc5efda729a00ee89c8215eb1ed5e14423/cache-7e2c1fe8be0206ad.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/11/2024 11:07:37 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/jupyter/datasphere/project/datasetscache/marulyanova___alma-r-preference-f/default-31a8eb8e26a3d2e8/0.0.0/e0b54ddc5efda729a00ee89c8215eb1ed5e14423/cache-7e2c1fe8be0206ad.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running CPO preprocessing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2009/2009 [00:01<00:00, 1265.21 examples/s]\n",
      "Caching indices mapping at /home/jupyter/datasphere/project/datasetscache/marulyanova___alma-r-preference-f/default-31a8eb8e26a3d2e8/0.0.0/e0b54ddc5efda729a00ee89c8215eb1ed5e14423/cache-2b8a52fd1455db45.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/11/2024 11:07:38 - INFO - datasets.arrow_dataset - Caching indices mapping at /home/jupyter/datasphere/project/datasetscache/marulyanova___alma-r-preference-f/default-31a8eb8e26a3d2e8/0.0.0/e0b54ddc5efda729a00ee89c8215eb1ed5e14423/cache-2b8a52fd1455db45.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:673] 2024-10-11 11:07:38,711 >> loading configuration file ALMA/configs/conf_axinia.json\n",
      "[INFO|configuration_utils.py:742] 2024-10-11 11:07:38,713 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"ALMA/configs/conf_axinia.json\",\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"compute_environment\": \"LOCAL_MACHINE\",\n",
      "  \"deepspeed_config\": {\n",
      "    \"gradient_accumulation_steps\": 1,\n",
      "    \"gradient_clipping\": 1.0,\n",
      "    \"offload_optimizer_device\": \"cuda\",\n",
      "    \"offload_param_device\": \"cpu\",\n",
      "    \"zero3_init_flag\": false,\n",
      "    \"zero_stage\": 2\n",
      "  },\n",
      "  \"distributed_type\": \"DEEPSPEED\",\n",
      "  \"downcast_bf16\": \"no\",\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"machine_rank\": 0,\n",
      "  \"main_training_function\": \"main\",\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"mixed_precision\": \"bf16\",\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"num_machines\": 1,\n",
      "  \"num_processes\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rdzv_backend\": \"static\",\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"same_network\": true,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tpu_env\": [],\n",
      "  \"tpu_use_cluster\": false,\n",
      "  \"tpu_use_sudo\": false,\n",
      "  \"transformers_version\": \"4.45.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_cpu\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3732] 2024-10-11 11:07:39,361 >> loading weights file model.safetensors from cache at /home/jupyter/datasphere/project/modelcache/models--haoranxu--X-ALMA-13B-Pretrain/snapshots/c57c245bbd2a9c165ea1da6c24f857d8107afcbf/model.safetensors.index.json\n",
      "Downloading shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:00<00:00,  6.05it/s]\n",
      "[INFO|configuration_utils.py:1099] 2024-10-11 11:07:40,357 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 512\n",
      "}\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Trying to set a tensor of shape torch.Size([32000, 5120]) in \"weight\" (which has shape torch.Size([32000, 4096])), this looks incorrect.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/work/resources/ALMA/run_cpo_llmmt.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/work/resources/ALMA/run_cpo_llmmt.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;31m# Load model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;31m# Initialize our Trainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/resources/ALMA/utils/utils.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(data_args, model_args, training_args, tokenizer, logger)\u001b[0m\n\u001b[1;32m    505\u001b[0m             )\n\u001b[1;32m    506\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m             model = AutoModelLoad.from_pretrained(\n\u001b[0m\u001b[1;32m    508\u001b[0m                 \u001b[0mmodel_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlast_checkpoint\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlast_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m                 \u001b[0mfrom_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".ckpt\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    565\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             )\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4012\u001b[0m                 \u001b[0moffload_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4013\u001b[0m                 \u001b[0merror_msgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4014\u001b[0;31m             \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_pretrained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4015\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4016\u001b[0m                 \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path)\u001b[0m\n\u001b[1;32m   4500\u001b[0m                                 )\n\u001b[1;32m   4501\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4502\u001b[0;31m                         new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(\n\u001b[0m\u001b[1;32m   4503\u001b[0m                             \u001b[0mmodel_to_load\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4504\u001b[0m                             \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys, pretrained_model_name_or_path)\u001b[0m\n\u001b[1;32m    971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0;31m# For backward compatibility with older versions of `accelerate` and for non-quantized params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m             \u001b[0mset_module_tensor_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mset_module_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m             \u001b[0mhf_quantizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_quantized_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/utils/modeling.py\u001b[0m in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;31m# In other cases, we want to make sure we're not loading checkpoints that do not match the config.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mold_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparam_cls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"Params4bit\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    287\u001b[0m                 \u001b[0;34mf'Trying to set a tensor of shape {value.shape} in \"{tensor_name}\" (which has shape {old_value.shape}), this looks incorrect.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             )\n",
      "\u001b[0;31mValueError\u001b[0m: Trying to set a tensor of shape torch.Size([32000, 5120]) in \"weight\" (which has shape torch.Size([32000, 4096])), this looks incorrect."
     ]
    }
   ],
   "source": [
    "%run ./ALMA/run_cpo_llmmt.py \\\n",
    "    --config_name ALMA/configs/conf_axinia.json \\\n",
    "    --model_name_or_path haoranxu/X-ALMA-13B-Pretrain \\\n",
    "    --tokenizer_name haoranxu/X-ALMA-13B-Pretrain \\\n",
    "    --peft_model_id  haoranxu/ALMA-13B-Pretrain-LoRA \\\n",
    "    --cpo_scorer kiwi_xcomet \\\n",
    "    --beta 0.1 \\\n",
    "    --use_peft \\\n",
    "    --use_fast_tokenizer False \\\n",
    "    --cpo_data_path  marulyanova/ALMA-R-Preference-F \\\n",
    "    --do_train \\\n",
    "    --language_pairs \"['ru-en']\" \\\n",
    "    --low_cpu_mem_usage \\\n",
    "    --bf16 \\\n",
    "    --learning_rate 1e-4 \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --gradient_accumulation_steps 1 \\\n",
    "    --lr_scheduler_type inverse_sqrt \\\n",
    "    --warmup_ratio 0.01 \\\n",
    "    --ignore_pad_token_for_loss \\\n",
    "    --ignore_prompt_token_for_loss \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --evaluation_strategy no \\\n",
    "    --save_strategy steps \\\n",
    "    --save_total_limit 1 \\\n",
    "    --logging_strategy steps \\\n",
    "    --logging_steps 0.05 \\\n",
    "    --output_dir dist \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --prediction_loss_only \\\n",
    "    --max_new_tokens 256 \\\n",
    "    --max_source_length 256 \\\n",
    "    --max_prompt_length 256 \\\n",
    "    --max_length 512 \\\n",
    "    --seed 42 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --report_to none \\\n",
    "    --overwrite_cache \\\n",
    "    --eval_accumulation_steps 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d87414-10e0-4c7c-ae67-dbf795e047bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
